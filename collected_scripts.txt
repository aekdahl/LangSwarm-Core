# Total Document Length: 57522 characters


--------------------------------------------------------------------------------
File: collect_scripts.py
--------------------------------------------------------------------------------

import os

# Function to write all files into a single output file
def collect_scripts(start_folder="START_FOLDER", output_file="collected_scripts.txt", include_root_files=False):
    # Determine the repository root and the start path
    repo_root = os.getenv("GITHUB_WORKSPACE", os.getcwd())
    start_path = os.path.join(repo_root, start_folder)

    if not os.path.exists(start_path):
        print(f"Error: The folder '{start_folder}' does not exist.")
        return

    total_length = 0  # Variable to track the total length of the document

    with open(output_file, "w", encoding="utf-8") as outfile:
        # Include files from the root folder of the repository if the flag is set
        if include_root_files:
            for file in os.listdir(repo_root):
                file_path = os.path.join(repo_root, file)
                if os.path.isfile(file_path) and not file.startswith("."):  # Exclude hidden files
                    try:
                        # Write the relative file name and path as a header
                        relative_path = os.path.relpath(file_path, repo_root)
                        outfile.write(f"\n{'-'*80}\n")
                        outfile.write(f"File: {relative_path}\n")
                        outfile.write(f"{'-'*80}\n\n")

                        # Read the file content and write it
                        with open(file_path, "r", encoding="utf-8") as infile:
                            content = infile.read()
                            total_length += len(content)
                            outfile.write(content)
                            outfile.write("\n\n")
                    except Exception as e:
                        # Log files that couldn't be read
                        error_message = f"[ERROR READING FILE: {file} - {e}]\n\n"
                        total_length += len(error_message)
                        outfile.write(error_message)

        # Walk through the specified folder and its subfolders
        for root, _, files in os.walk(start_path):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, start_path)

                try:
                    # Write the relative file name and path as a header
                    outfile.write(f"\n{'-'*80}\n")
                    outfile.write(f"File: {relative_path}\n")
                    outfile.write(f"{'-'*80}\n\n")

                    # Read the file content and write it
                    with open(file_path, "r", encoding="utf-8") as infile:
                        content = infile.read()
                        total_length += len(content)
                        outfile.write(content)
                        outfile.write("\n\n")
                except Exception as e:
                    # Log files that couldn't be read
                    error_message = f"[ERROR READING FILE: {relative_path} - {e}]\n\n"
                    total_length += len(error_message)
                    outfile.write(error_message)

    # Prepend the total document length to the file
    prepend_length_comment(output_file, total_length)


def prepend_length_comment(output_file, total_length):
    """
    Prepend a comment with the total document length to the output file.
    """
    with open(output_file, "r+", encoding="utf-8") as f:
        content = f.read()
        f.seek(0, 0)  # Move to the start of the file
        f.write(f"# Total Document Length: {total_length} characters\n\n")
        f.write(content)


if __name__ == "__main__":
    output_filename = "collected_scripts.txt"
    start_folder = "langswarm"  # Specify the start folder (e.g., "/" for the root of the repository)
    include_root = True  # Set this flag to True to include files in the repository root folder
    print(f"Collecting scripts from '{start_folder}' into {output_filename}...")
    collect_scripts(start_folder=start_folder, output_file=output_filename, include_root_files=include_root)
    print(f"All scripts have been collected into {output_filename}.")



--------------------------------------------------------------------------------
File: README.md
--------------------------------------------------------------------------------

# LangSwarm-Core

LangSwarm-Core is a framework designed to support multi-agent systems using Large Language Models (LLMs). It provides utilities, memory management, logging integration, and agent orchestration tools to build robust AI ecosystems with modularity and flexibility.

## Features

- **Agent Wrappers**: Easily integrate with LangChain, OpenAI, Hugging Face, and LlamaIndex agents.
- **Memory Management**: Support for in-memory or external memory, with customizable options.
- **Logging**: Seamless integration with LangSmith for advanced logging and tracing.
- **Factory Design**: Create and manage multiple agents with configurable setups.
- **Utilities**: Helper functions for token management, text cleaning, and cost estimation.
- **Registry**: Centralized agent registry to manage and access agents dynamically.

---

## Installation

### Prerequisites
- Python 3.8 or higher
- Install dependencies:
  ```bash
  pip install -r requirements.txt
  ```

### From PyPI
  ```bash
  pip install langswarm-core
  ```

---

## Usage

### Quick Start

Here's an example of how to use LangSwarm-Core to create an agent and interact with it:

```python
from core.factory.agents import AgentFactory

# Create a LangChain agent
agent = AgentFactory.create(
    name="example_agent",
    agent_type="langchain-openai",
    memory=[],
    langsmith_api_key="your-langsmith-api-key",
    model="gpt-4"
)

# Use the agent to respond to queries
response = agent.chat("What is LangSwarm?")
print(response)
```

### Memory Integration

LangSwarm-Core supports memory out of the box. Here's how to initialize an agent with memory:

```python
from core.factory.agents import AgentFactory

memory = []  # Initialize in-memory storage

agent = AgentFactory.create(
    name="memory_agent",
    agent_type="langchain-openai",
    memory=memory,
    model="gpt-4"
)

response = agent.chat("Remember this: LangSwarm is awesome.")
print(response)
```

---

## Components

### Wrappers
Wrappers add modular capabilities such as:
- Memory management (`MemoryMixin`)
- Logging integration (`LoggingMixin`)

### Utilities
Helper functions for:
- Token and cost estimation
- Text processing
- JSON and YAML validation

### Factory
Use the `AgentFactory` to easily create and configure agents:
```python
agent = AgentFactory.create(
    name="example",
    agent_type="llamaindex",
    documents=["Sample text for indexing"]
)
```

---

## Development

### Setting Up the Environment
1. Clone the repository:
   ```bash
   git clone https://github.com/aekdahl/langswarm-core.git
   cd langswarm-core
   ```
2. Create a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running Tests
Tests are located in the `tests/` directory. Run them using `pytest`:
```bash
pytest
```

---

## Contributing

We welcome contributions! To get started:
1. Fork the repository.
2. Create a feature branch.
3. Make your changes and write tests.
4. Submit a pull request.

---

## Roadmap

- Add support for additional LLM providers.
- Expand orchestration capabilities with reinforcement learning agents.
- Develop CLI tools for managing agents.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

LangSwarm-Core relies on several amazing libraries, including:
- [LangChain](https://github.com/hwchase17/langchain)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)

---

Feel free to modify this to better fit your specific project details or branding!



--------------------------------------------------------------------------------
File: collected_scripts.txt
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
File: collect_scripts.py
--------------------------------------------------------------------------------

import os

# Function to write all files into a single output file
def collect_scripts(start_folder="START_FOLDER", output_file="collected_scripts.txt", include_root_files=False):
    # Determine the repository root and the start path
    repo_root = os.getenv("GITHUB_WORKSPACE", os.getcwd())
    start_path = os.path.join(repo_root, start_folder)

    if not os.path.exists(start_path):
        print(f"Error: The folder '{start_folder}' does not exist.")
        return

    total_length = 0  # Variable to track the total length of the document

    with open(output_file, "w", encoding="utf-8") as outfile:
        # Include files from the root folder of the repository if the flag is set
        if include_root_files:
            for file in os.listdir(repo_root):
                file_path = os.path.join(repo_root, file)
                if os.path.isfile(file_path) and not file.startswith("."):  # Exclude hidden files
                    try:
                        # Write the relative file name and path as a header
                        relative_path = os.path.relpath(file_path, repo_root)
                        outfile.write(f"\n{'-'*80}\n")
                        outfile.write(f"File: {relative_path}\n")
                        outfile.write(f"{'-'*80}\n\n")

                        # Read the file content and write it
                        with open(file_path, "r", encoding="utf-8") as infile:
                            content = infile.read()
                            total_length += len(content)
                            outfile.write(content)
                            outfile.write("\n\n")
                    except Exception as e:
                        # Log files that couldn't be read
                        error_message = f"[ERROR READING FILE: {file} - {e}]\n\n"
                        total_length += len(error_message)
                        outfile.write(error_message)

        # Walk through the specified folder and its subfolders
        for root, _, files in os.walk(start_path):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, start_path)

                try:
                    # Write the relative file name and path as a header
                    outfile.write(f"\n{'-'*80}\n")
                    outfile.write(f"File: {relative_path}\n")
                    outfile.write(f"{'-'*80}\n\n")

                    # Read the file content and write it
                    with open(file_path, "r", encoding="utf-8") as infile:
                        content = infile.read()
                        total_length += len(content)
                        outfile.write(content)
                        outfile.write("\n\n")
                except Exception as e:
                    # Log files that couldn't be read
                    error_message = f"[ERROR READING FILE: {relative_path} - {e}]\n\n"
                    total_length += len(error_message)
                    outfile.write(error_message)

    # Prepend the total document length to the file
    prepend_length_comment(output_file, total_length)


def prepend_length_comment(output_file, total_length):
    """
    Prepend a comment with the total document length to the output file.
    """
    with open(output_file, "r+", encoding="utf-8") as f:
        content = f.read()
        f.seek(0, 0)  # Move to the start of the file
        f.write(f"# Total Document Length: {total_length} characters\n\n")
        f.write(content)


if __name__ == "__main__":
    output_filename = "collected_scripts.txt"
    start_folder = "langswarm"  # Specify the start folder (e.g., "/" for the root of the repository)
    include_root = True  # Set this flag to True to include files in the repository root folder
    print(f"Collecting scripts from '{start_folder}' into {output_filename}...")
    collect_scripts(start_folder=start_folder, output_file=output_filename, include_root_files=include_root)
    print(f"All scripts have been collected into {output_filename}.")



--------------------------------------------------------------------------------
File: README.md
--------------------------------------------------------------------------------

# LangSwarm-Core

LangSwarm-Core is a framework designed to support multi-agent systems using Large Language Models (LLMs). It provides utilities, memory management, logging integration, and agent orchestration tools to build robust AI ecosystems with modularity and flexibility.

## Features

- **Agent Wrappers**: Easily integrate with LangChain, OpenAI, Hugging Face, and LlamaIndex agents.
- **Memory Management**: Support for in-memory or external memory, with customizable options.
- **Logging**: Seamless integration with LangSmith for advanced logging and tracing.
- **Factory Design**: Create and manage multiple agents with configurable setups.
- **Utilities**: Helper functions for token management, text cleaning, and cost estimation.
- **Registry**: Centralized agent registry to manage and access agents dynamically.

---

## Installation

### Prerequisites
- Python 3.8 or higher
- Install dependencies:
  ```bash
  pip install -r requirements.txt
  ```

### From PyPI
  ```bash
  pip install langswarm-core
  ```

---

## Usage

### Quick Start

Here's an example of how to use LangSwarm-Core to create an agent and interact with it:

```python
from core.factory.agents import AgentFactory

# Create a LangChain agent
agent = AgentFactory.create(
    name="example_agent",
    agent_type="langchain-openai",
    memory=[],
    langsmith_api_key="your-langsmith-api-key",
    model="gpt-4"
)

# Use the agent to respond to queries
response = agent.chat("What is LangSwarm?")
print(response)
```

### Memory Integration

LangSwarm-Core supports memory out of the box. Here's how to initialize an agent with memory:

```python
from core.factory.agents import AgentFactory

memory = []  # Initialize in-memory storage

agent = AgentFactory.create(
    name="memory_agent",
    agent_type="langchain-openai",
    memory=memory,
    model="gpt-4"
)

response = agent.chat("Remember this: LangSwarm is awesome.")
print(response)
```

---

## Components

### Wrappers
Wrappers add modular capabilities such as:
- Memory management (`MemoryMixin`)
- Logging integration (`LoggingMixin`)

### Utilities
Helper functions for:
- Token and cost estimation
- Text processing
- JSON and YAML validation

### Factory
Use the `AgentFactory` to easily create and configure agents:
```python
agent = AgentFactory.create(
    name="example",
    agent_type="llamaindex",
    documents=["Sample text for indexing"]
)
```

---

## Development

### Setting Up the Environment
1. Clone the repository:
   ```bash
   git clone https://github.com/aekdahl/langswarm-core.git
   cd langswarm-core
   ```
2. Create a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running Tests
Tests are located in the `tests/` directory. Run them using `pytest`:
```bash
pytest
```

---

## Contributing

We welcome contributions! To get started:
1. Fork the repository.
2. Create a feature branch.
3. Make your changes and write tests.
4. Submit a pull request.

---

## Roadmap

- Add support for additional LLM providers.
- Expand orchestration capabilities with reinforcement learning agents.
- Develop CLI tools for managing agents.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

LangSwarm-Core relies on several amazing libraries, including:
- [LangChain](https://github.com/hwchase17/langchain)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)

---

Feel free to modify this to better fit your specific project details or branding!





--------------------------------------------------------------------------------
File: setup.cfg
--------------------------------------------------------------------------------

[metadata]
description-file = README.md

[options]
packages = find:
python_requires = >=3.8

[options.extras_require]
dev =
    pytest
    black
    flake8



--------------------------------------------------------------------------------
File: pyproject.toml
--------------------------------------------------------------------------------

[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"



--------------------------------------------------------------------------------
File: MANIFEST.in
--------------------------------------------------------------------------------

include README.md
include LICENSE
include requirements.txt
recursive-include core *
recursive-exclude __pycache__ *
recursive-exclude *.pyc



--------------------------------------------------------------------------------
File: LICENSE
--------------------------------------------------------------------------------

MIT License

Copyright (c) 2025 Alexander Ekdahl

Permission is hereby granted, free of charge, to any person obtaining a copy of this software...



--------------------------------------------------------------------------------
File: setup.py
--------------------------------------------------------------------------------

from setuptools import setup, find_packages

setup(
    name="langswarm-core",
    version="0.0.16",
    description="A core framework for multi-agent LLM ecosystems",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/aekdahl/langswarm-core",
    author="Alexander Ekdahl",
    author_email="alexander.ekdahl@gmail.com",
    license="MIT",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
    packages=find_packages(),
    python_requires=">=3.8",
    install_requires=[
        "langchain",
        "tiktoken",
        "llama-index",
        "pyyaml",
        # Add other dependencies here
    ],
    extras_require={
        "dev": ["pytest", "black", "flake8"],
    },
    include_package_data=True,
    entry_points={
        "console_scripts": [
            # If your package includes CLI tools, specify them here.
            # e.g., "langswarm=core.cli:main",
        ],
    },
)



--------------------------------------------------------------------------------
File: requirements.txt
--------------------------------------------------------------------------------

# Supported versions of Python: 3.10, 3.11, 3.8, 3.9
# Automatically updated by dependency_update_test.py

langchain==0.2.17
tiktoken==0.7.0
llama-index==0.11.23
pyyaml==6.0.2



--------------------------------------------------------------------------------
File: dependency_update_test.py
--------------------------------------------------------------------------------

import subprocess
import requests
import sys
from packaging.version import Version


def fetch_versions(package_name):
    """
    Fetch all available versions of a package from PyPI.
    """
    url = f"https://pypi.org/pypi/{package_name}/json"
    try:
        response = requests.get(url)
        response.raise_for_status()
        all_versions = list(response.json()["releases"].keys())
        # Sort versions using `packaging.version.Version`
        all_versions.sort(key=Version)
        return all_versions
    except requests.RequestException as e:
        print(f"Error fetching versions for {package_name}: {e}")
        return []


def test_dependency_version(package, version):
    """
    Test if a specific version of a package can be installed.
    """
    try:
        print(f"Testing {package}=={version}...")
        subprocess.run(
            [sys.executable, "-m", "pip", "install", f"{package}=={version}"],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        print(f"{package}=={version} installed successfully!")
        return True
    except subprocess.CalledProcessError:
        print(f"{package}=={version} failed.")
        return False


def find_oldest_compatible_version(package, versions):
    """
    Find the oldest compatible version of a package by testing all versions.
    """
    compatible_version = None
    for version in reversed(versions):  # Test oldest versions first
        if test_dependency_version(package, version):
            compatible_version = version
            break
    return compatible_version


def get_supported_python_versions():
    """
    Extract the list of supported Python versions from the requirements.txt file.
    """
    supported_versions = []
    try:
        with open("requirements.txt", "r") as f:
            for line in f:
                if line.startswith("# Supported versions of Python:"):
                    supported_versions = line.strip().split(":")[1].strip().split(", ")
                    break
    except FileNotFoundError:
        pass
    return supported_versions


def update_requirements_with_python_versions(dependency_versions, python_version, success):
    """
    Update the requirements.txt file with the latest compatible versions
    and maintain only supported Python versions.
    """
    # Get existing supported versions
    supported_versions = set(get_supported_python_versions())

    if success:
        supported_versions.add(python_version)  # Add the Python version if it succeeded
    else:
        supported_versions.discard(python_version)  # Remove the version if it failed

    # Sort for consistency
    supported_versions = sorted(supported_versions)

    with open("requirements.txt", "w") as f:
        # Add the comment about supported Python versions
        f.write(f"# Supported versions of Python: {', '.join(supported_versions)}\n")
        f.write("# Automatically updated by dependency_update_test.py\n\n")

        # Write the compatible dependency versions
        for package, compatible_version in dependency_versions.items():
            f.write(f"{package}=={compatible_version}\n")
    print("requirements.txt updated successfully with Python version support comment.")


def main(python_version):
    # Read dependencies from requirements.txt
    try:
        with open("requirements.txt", "r") as f:
            dependencies = [line.strip().split("==")[0] for line in f if "==" in line]
    except FileNotFoundError:
        print("requirements.txt not found.")
        sys.exit(1)

    latest_versions = {}
    success = True  # Track whether all tests passed
    for package in dependencies:
        print(f"\nFetching versions for {package}...")
        versions = fetch_versions(package)
        if not versions:
            print(f"No versions found for {package}. Skipping...")
            continue

        print(f"Available versions for {package}: {versions}")
        compatible_version = find_oldest_compatible_version(package, versions)
        if compatible_version:
            print(f"Oldest compatible version for {package}: {compatible_version}")
            latest_versions[package] = compatible_version
        else:
            print(f"No compatible version found for {package} on Python {python_version}.")
            success = False
            break  # Exit the loop and mark the test as failed

    # Update requirements.txt with compatible versions and supported Python versions
    update_requirements_with_python_versions(latest_versions, python_version, success)

    if not success:
        sys.exit(1)  # Exit with failure if any dependency test failed


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python dependency_update_test.py <python_version>")
        sys.exit(1)
    main(sys.argv[1])



--------------------------------------------------------------------------------
File: __init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/wrappers/memory_mixin.py
--------------------------------------------------------------------------------

from typing import Any, Optional

try:
    from langchain.memory import BaseMemory
except ImportError:
    BaseMemory = None

class MemoryMixin:
    """
    Mixin for memory management.
    """

    def _initialize_memory(self, agent: Any, memory: Optional[Any], in_memory: list) -> Optional[Any]:
        """
        Initialize or validate memory for the agent.
        """
        if hasattr(agent, "memory") and agent.memory:
            return agent.memory

        if memory:
            if BaseMemory and isinstance(memory, BaseMemory):
                return memory
            raise ValueError("Invalid memory instance provided.")

        return None



--------------------------------------------------------------------------------
File: core/wrappers/logging_mixin.py
--------------------------------------------------------------------------------

from typing import Any, Optional
import logging

try:
    from langsmith.tracers.helpers import traceable, log_error
    from langsmith.wrappers import wrap_openai
    from langsmith import LangSmithTracer
except ImportError:
    traceable = None
    log_error = None
    wrap_openai = None
    LangSmithTracer = None


class LoggingMixin:
    """
    Mixin for managing LangSmith logging and fallback logging.
    """

    def _initialize_logger(self, name: str, langsmith_api_key: Optional[str]) -> logging.Logger:
        """
        Initialize a logger for the agent, with LangSmith integration if available.

        Parameters:
        - name (str): The name of the logger.
        - langsmith_api_key (str): API key for LangSmith, if enabled.

        Returns:
        - Logger instance.
        """
        self.langsmith_enabled = langsmith_api_key is not None and traceable is not None
        self.langsmith_tracer = None

        if self.langsmith_enabled and LangSmithTracer:
            self.langsmith_tracer = LangSmithTracer(api_key=langsmith_api_key)

        logger = logging.getLogger(name)
        if not logger.hasHandlers():
            handler = logging.StreamHandler()
            formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger

    def _log_error(self, error_message: str):
        """
        Log errors to LangSmith or fallback logger.

        Parameters:
        - error_message (str): The error message to log.
        """
        if self.langsmith_enabled and self.langsmith_tracer and log_error:
            self.langsmith_tracer.log_error(error_message, name=self.name, run_type="error")
        else:
            self.logger.error(f"Error in {self.name}: {error_message}")



--------------------------------------------------------------------------------
File: core/wrappers/generic.py
--------------------------------------------------------------------------------

from typing import Any, Optional

from ..base.bot import LLM
from .base_wrapper import BaseWrapper
from .logging_mixin import LoggingMixin
from .memory_mixin import MemoryMixin


class AgentWrapper(LLM, BaseWrapper, LoggingMixin, MemoryMixin):
    """
    A unified wrapper for LLM agents, combining memory management, logging, and LangSmith integration.
    """

    def __init__(self, name, agent, memory=None, is_conversational=False, langsmith_api_key=None, **kwargs):
        kwargs.pop("provider", None)  # Remove `provider` if it exists
        super().__init__(name=name, agent=agent, provider="wrapper", **kwargs)
        
        self.logger = self._initialize_logger(name, langsmith_api_key)
        self.memory = self._initialize_memory(agent, memory, self.in_memory)
        self.is_conversational = is_conversational

    def chat(self, q=None, reset=False, erase_query=False, remove_linebreaks=False):
        """
        Process a query using the wrapped agent.

        Parameters:
        - q (str): Query string.
        - reset (bool): Whether to reset memory before processing.
        - erase_query (bool): Whether to erase the query after processing.
        - remove_linebreaks (bool): Remove line breaks from the query.

        Returns:
        - str: The agent's response.
        """
        if reset:
            self.in_memory = []
            if self.memory and hasattr(self.memory, clear):
                self.memory.clear()

        if q:
            self.add_message(q, role="user", remove_linebreaks=remove_linebreaks)
            self.logger.info(f"Query sent to agent {self.name}: {q}")

        try:
            # Handle different agent types
            if hasattr(self.agent, "run"):
                # LangChain agents
                response = self.agent.run(q)
                if hasattr(self.agent, "memory") and self.agent.memory:
                    # Memory is already managed by the agent
                    response = self.agent.run(q)
                else:
                    # No memory, include context manually
                    context = " ".join([message["content"] for message in self.in_memory]) if self.in_memory else ""
                    full_query = f"{context}\n{q}"
                    response = self.agent.run(full_query)
            elif self._is_llamaindex_agent(self.agent):
                # LlamaIndex agents
                context = " ".join([message["content"] for message in self.in_memory])
                response = self.agent.query(context if self.memory else q).response
            elif callable(self.agent):
                # Hugging Face agents
                context = " ".join([message["content"] for message in self.in_memory]) if self.is_conversational else q
                response = self.agent(context)
            elif self._is_openai_llm(self.agent) or hasattr(self.agent, "ChatCompletion"):
                try:
                    completion = self.agent.ChatCompletion.create(
                        model=self.model,
                        messages=self.in_memory,
                        temperature=0.0
                    )
                    response = completion['choices'][0]['message']['content']
                except:
                    completion = self.agent.chat.completions.create(
                        model=self.model,
                        messages=self.in_memory,
                        temperature=0.0
                    )
                    response = completion.choices[0].message.content
            else:
                raise ValueError(f"Unsupported agent type: {type(self.agent)} for agent: {self.agent}")

            # Parse and log response
            response = self._parse_response(response)
            self.logger.info(f"Agent {self.name} response: {response}")

            if erase_query:
                self.remove()

            return response

        except Exception as e:
            self._log_error(str(e))
            raise

    def _parse_response(self, response: Any) -> str:
        """
        Parse the response from the wrapped agent.

        Parameters:
        - response: The agent's raw response.

        Returns:
        - str: The parsed response.
        """
        if hasattr(response, "content"):
            return response.content
        elif isinstance(response, dict):
            return response.get("generated_text", "")
        return str(response)

    def __getattr__(self, name: str) -> Any:
        """
        Delegate attribute access to the wrapped agent.

        Parameters:
        - name (str): The attribute name.

        Returns:
        - The attribute from the wrapped agent.
        """
        return getattr(self.agent, name)



--------------------------------------------------------------------------------
File: core/wrappers/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/wrappers/base_wrapper.py
--------------------------------------------------------------------------------

from ..registry.agents import AgentRegistry
from typing import Any, Optional

class BaseWrapper:
    """
    Base class for wrapping agents, providing initialization and validation.
    """

    def __init__(self, name: str, agent: Any, **kwargs):
        self.name = name
        self.agent = agent
        self.kwargs = kwargs

        # Register the agent in the global registry
        AgentRegistry.register(
            name=name,
            agent=self,
            agent_type=type(agent).__name__,
            metadata=kwargs.get('metadata', {})
        )

    @staticmethod
    def _get_module_path(module_class: Any) -> str:
        """
        Returns the full module path of a given class or callable.
        :param module_class: The class or callable to get the module path for.
        :type module_class: Any
        :return: The module path
        :rtype: str
        """
        return (
            getattr(module_class, "__module__", "")
            + "."
            + getattr(module_class, "__name__", "")
        ).strip(".")
        
    @staticmethod
    def _is_openai_llm(agent: Any) -> bool:
        """
        Determine if the agent is an OpenAI LLM.
        Parameters:
        - agent: The agent to check.
        Returns:
        - bool: True if the agent is an OpenAI LLM, False otherwise.
        """
        return hasattr(agent, "model") and "openai" in str(type(agent)).lower()

    @staticmethod
    def _is_llamaindex_agent(agent):
        """
        Determine if the given agent is a LlamaIndex agent.
    
        Parameters:
        - agent: The agent to check.
    
        Returns:
        - bool: True if the agent is a LlamaIndex agent, False otherwise.
        """
        module_name = getattr(agent.__class__, "__module__", "")
        if "llama_index" in module_name:
            return True
        return callable(getattr(agent, "query", None))

    def _validate_agent(self):
        if not callable(self.agent) and not hasattr(self.agent, "run"):
            raise ValueError(f"Unsupported agent type: {type(self.agent)}")



--------------------------------------------------------------------------------
File: core/utils/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/utils/utilities.py
--------------------------------------------------------------------------------

import time
import hashlib
from io import StringIO
from html.parser import HTMLParser

# pip install pyyaml
import yaml
import ast
import uuid
import json
import re
import os

#%pip install --upgrade tiktoken
import tiktoken

class Utils:
    def __init__(self):
        self.total_tokens_estimate = 0
        self.total_price_estimate = 0
        self.bot_logs = []

    def _get_api_key(self, provider, api_key):
        """
        Retrieve the API key from environment variables or fallback to the provided key.

        Args:
            provider (str): LLM provider.
            api_key (str): Provided API key.

        Returns:
            str: Resolved API key.
        """
        env_var_map = {
            "langchain-openai": "OPENAI_API_KEY",
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "google": "GOOGLE_PALM_API_KEY",
        }
        env_var = env_var_map.get(provider.lower())

        if env_var and (key_from_env := os.getenv(env_var)):
            return key_from_env

        if api_key:
            return api_key

        raise ValueError(f"API key for {provider} not found. Set {env_var} or pass the key explicitly.")
        
    def bot_log(self, bot, message):
        self.bot_logs.append((bot, message))

    def print_current_estimates(self):
        print('Estimated total tokens:', self.total_tokens_estimate)
        print('Estimated total price: $', self.total_price_estimate)

    def update_price_tokens_use_estimates(self, string, model = 'gpt-4-1106-preview', price = 0.150, verbose = False):
        tokens, price = self.price_tokens_from_string(string, model, price, verbose)
        self.total_tokens_estimate = self.total_tokens_estimate + tokens
        self.total_price_estimate = self.total_price_estimate + price
        if verbose:
            self.print_current_estimates()

    def price_tokens_from_string(self, string, encoding_name, price = 0.150, verbose = False):
        """Returns the number of tokens in a text string."""
        try:
            encoding = tiktoken.encoding_for_model(encoding_name)
        except:
            encoding = tiktoken.encoding_for_model('gpt-4-1106-preview')
        num_tokens = len(encoding.encode(string))
        price = round(num_tokens*price/1000000, 4)
        if verbose:
            print('Estimated tokens:', num_tokens)
            print('Estimated price: $', price)
        
        return num_tokens, price

    def is_valid_json(self, json_string):
        try:
            json.loads(json_string)
        except ValueError:
            return False

        return True

    def is_valid_python(self, code):
        try:
            ast.parse(code)
        except SyntaxError:
            return False

        return True

    def is_valid_yaml(self, code):
        try:
            yaml.load(code)
        except yaml.YAMLError:
            return False

        return True
    
    def clear_markdown(self, text):
        
        # Remove starting code markup
        if text.startswith('```python'):
            text = text.split('```python',1)[-1]
        elif text.startswith('```json'):
            text = text.split('```json',1)[-1]
        elif text.startswith('```yaml'):
            text = text.split('```yaml',1)[-1]
        elif text.startswith('```plaintext'):
            text = text.split('```plaintext',1)[-1]
        elif text.startswith('```javascript'):
            text = text.split('```javascript',1)[-1]
        elif text.startswith('```html'):
            text = text.split('```html',1)[-1]
        elif text.startswith('```css'):
            text = text.split('```css',1)[-1]
        elif text.startswith('```'):
            text = text.split('```',1)[-1]

        # Remove ending code markup
        if text.endswith('```'):
            text = text.rsplit('```',1)[0]

        return text

    def clean_text(self, text, remove_linebreaks = False):
        txt = text.encode('ascii', 'ignore').decode()
        txt = txt.replace('\\n',' ')
        if remove_linebreaks:
            txt = txt.replace('\n',' ')
        return txt.replace('\\u00a0',' ')

    def strip_tags(self, text, remove_linebreaks = False):
        strip_tags = StripTags()
        strip_tags.reset()
        strip_tags.feed(text)
        txt = strip_tags.get_data().encode('ascii', 'ignore').decode()
        txt = txt.replace('\\n',' ')
        if remove_linebreaks:
            txt = txt.replace('\n',' ')
        return txt.replace('\\u00a0',' ')
    
    def generate_short_uuid(self, length = 8):
        # Generate a UUID and return a shortened version (min. 2 characters)
        return 'z'+str(uuid.uuid4())[:max(1,length-1)]
    
    def generate_md5_hash(self, query):
        return hashlib.md5(str(query).encode('utf-8')).hexdigest()
    
    def safe_str_to_int(self, s):
        # Extract numeric part using regex
        match = re.search(r"[-+]?\d*\.?\d+", s)
        if match:
            return int(match.group())
        return 0  # Return 0 if no valid number is found


class StripTags(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs= True
        self.text = StringIO()
    def handle_data(self, d):
        self.text.write(d)
    def get_data(self):
        return self.text.getvalue()

class SafeMap(dict):
    def __missing__(self, key):
        return f'{{{key}}}'



--------------------------------------------------------------------------------
File: core/factory/agents.py
--------------------------------------------------------------------------------

from typing import Any, Optional
from ..wrappers.generic import AgentWrapper
from ..utils.utilities import Utils

try:
    from llama_index import GPTSimpleVectorIndex, Document
except ImportError:
    GPTSimpleVectorIndex = None
    Document = None

class AgentFactory:
    """
    A factory for creating LangSwarm agents, including LangChain, Hugging Face, OpenAI, and LlamaIndex agents.
    """

    @staticmethod
    def create(
        name: str,
        agent_type: str,
        documents: Optional[list] = None,
        memory: Optional[Any] = None,
        langsmith_api_key: Optional[str] = None,
        **kwargs,
    ) -> AgentWrapper:
        """
        Create an agent with the given parameters.

        Parameters:
        - name (str): The name of the agent.
        - agent_type (str): The type of agent ("langchain", "huggingface", "openai", "llamaindex", etc.).
        - documents (list, optional): Documents for LlamaIndex agents.
        - memory (optional): A memory instance to use with the agent.
        - langsmith_api_key (str, optional): API key for LangSmith logging.
        - kwargs: Additional parameters for the agent.

        Returns:
        - AgentWrapper: A wrapped agent ready for use.
        """
        agent = None
        utils = Utils()

        if agent_type.lower() == "llamaindex":
            if GPTSimpleVectorIndex is None or Document is None:
                raise ImportError("LlamaIndex is not installed. Install it with 'pip install llama-index'.")
            if not documents:
                raise ValueError("Documents must be provided to create a LlamaIndex agent.")
            doc_objects = [Document(text=doc) for doc in documents]
            agent = GPTSimpleVectorIndex(doc_objects)

        elif agent_type.lower() == "langchain-openai":
            # Example: Create a LangChain agent (e.g., OpenAI model)
            from langchain.llms import OpenAI
            model = kwargs.get("model", "gpt-3.5-turbo")
            api_key = utils._get_api_key('langchain-openai', kwargs.get("openai_api_key"))
            agent = OpenAI(model=model, openai_api_key=api_key)

        elif agent_type.lower() == "langchain":
            # Example: Create a LangChain agent (e.g., OpenAI model)
            from langchain.llms import OpenAI
            model = kwargs.get("model", "gpt-3.5-turbo")
            api_key = utils._get_api_key('langchain', kwargs.get("openai_api_key"))
            agent = OpenAI(model=model, openai_api_key=api_key)

        elif agent_type.lower() == "huggingface":
            # Example: Create a Hugging Face agent
            from transformers import pipeline
            task = kwargs.get("task", "text-generation")
            model = kwargs.get("model", "gpt2")
            agent = pipeline(task, model=model)

        elif agent_type.lower() == "openai":
            # Example: Create an OpenAI agent directly
            try:
                import openai
            except ImportError:
                raise ImportError(
                    "OpenAI is not available. Please install it:\n"
                    "  pip install openai"
                )
                
            openai.api_key = utils._get_api_key('openai', kwargs.get("openai_api_key"))
            agent = openai
        else:
            raise ValueError(f"Unsupported agent type: {agent_type}")

        # Wrap the agent using AgentWrapper
        return AgentWrapper(
            name=name,
            agent=agent,
            memory=memory,
            langsmith_api_key=langsmith_api_key,
            **kwargs,
        )



--------------------------------------------------------------------------------
File: core/factory/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/registry/agents.py
--------------------------------------------------------------------------------

# agents.py
class AgentRegistry:
    """
    Centralized registry for managing available LLM agents.
    """

    _registry = {}

    @classmethod
    def register(cls, name, agent, agent_type, metadata=None):
        """
        Register an agent with the registry.

        Parameters:
        - name (str): Unique identifier for the agent.
        - agent: The agent instance to register.
        - agent_type (str): Type of the agent (e.g., 'langchain', 'huggingface').
        - metadata (dict): Optional additional metadata about the agent.
        """
        cls._registry[name] = {
            "agent": agent,
            "type": agent_type,
            "metadata": metadata or {},
        }

    @classmethod
    def get(cls, name):
        """
        Retrieve an agent's details by name.

        Parameters:
        - name (str): The name of the agent.

        Returns:
        - dict: Agent details, or None if not found.
        """
        return cls._registry.get(name)

    @classmethod
    def list(cls):
        """
        List all registered agents.

        Returns:
        - dict: All agents in the registry.
        """
        return cls._registry

    @classmethod
    def remove(cls, name):
        """
        Remove an agent from the registry.

        Parameters:
        - name (str): The name of the agent.
        """
        if name in cls._registry:
            del cls._registry[name]



--------------------------------------------------------------------------------
File: core/registry/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/base/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: core/base/bot.py
--------------------------------------------------------------------------------


import os
import json
import logging
from types import SimpleNamespace

try:
    from langchain.memory import BaseMemory
except ImportError:
    BaseMemory = None  # Fallback if BaseMemory is not available

from ..utils.utilities import Utils

class LLM:
    """
    A class to interact with Large Language Models (LLMs) using LangChain or OpenAI.
    
    Provides methods for managing conversations, sending queries, and handling memory.
    """
    
    def __init__(
        self, 
        provider='langchain-openai',
        model='gpt-4o-mini-2024-07-18', 
        api_key=None,
        temperature=0.0,
        response_format='auto',
        system_prompt=None,
        utils=None,
        verbose=False,
        name=None, 
        team=None,
        memory=None,
        specialization=None,
        agent=None,
        **kwargs
    ):
        """
        Initialize the LLM instance.

        Args:
            provider (str): LLM provider ('langchain-openai' or 'openai').
            model (str): Model name to use.
            api_key (str): API key for the provider.
            temperature (float): Sampling temperature for response variability.
            response_format (str): Expected format of the response.
            system_prompt (str): Initial system prompt to set the model context.
            utils (Utils): Utility instance for logging and text processing.
            verbose (bool): Verbosity flag for debugging and logs.
            name (str): Name of the bot.
            team (str): Associated team for the bot.
            specialization (str): Bot's area of expertise.
        """
        self.agent = agent or SimpleNamespace()
        self.provider = provider
        self.utils = utils or Utils()

        if provider == 'langchain-openai':
            self.api_key = self.utils._get_api_key(provider, api_key)
            try:
                from langchain.chat_models import ChatOpenAI
            except ImportError:
                raise ImportError(
                    "Neither LangChain nor OpenAI is available. Please install them:\n"
                    "  pip install langchain langchain-openai"
                )
            self.agent = ChatOpenAI(model=model, openai_api_key=self.api_key, temperature=temperature)
        elif provider == 'openai':
            self.api_key = self.utils._get_api_key(provider, api_key)
            try:
                import openai
            except ImportError:
                raise ImportError(
                    "OpenAI is not available. Please install it:\n"
                    "  pip install openai"
                )
            openai.api_key = self.api_key
            self.agent = openai
        elif provider == 'wrapper':
            try:
                # Used for OpenAI
                self.agent.api_key = self.utils._get_api_key('openai', api_key)
            except:
                pass
            self.api_key = api_key
            team = team or 'wrappers'
        else:
            raise ValueError(f"Unsupported provider: {provider}.")

        
        self.memory = memory if not isinstance(memory, list) else None
        self.in_memory = memory if isinstance(memory, list) else []
        self.model = model
        self.name = name
        self.team = team
        self.last_in_memory = ''
        self.verbose = verbose
        self.system_prompt = system_prompt
        self.specialization = specialization

        self.update_system_prompt()
        self.utils.bot_log(self.name, {"role": "admin", "content": "Bot was created."})
        
        self.logger = logging.getLogger("LangSwarm.Bot")
        if not self.logger.hasHandlers():
            handler = logging.StreamHandler()
            formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

        self.logger.info("Bot logger initialized.")
    
    def update_system_prompt(self, system_prompt=None):
        """
        Update the system prompt for the conversation.

        Args:
            system_prompt (str): New system prompt to set.
        """
        if system_prompt:
            self.system_prompt = system_prompt

        if self.system_prompt:
            if self.memory:
                if hasattr(self.memory, "messages"):
                    messages = self.memory.messages  # Fetch current messages
                    if len(messages) > 0:
                        messages[0] = {"role": "system", "content": self.system_prompt}
                    else:
                        messages = [{"role": "system", "content": self.system_prompt}]
                    self.memory.clear()       # Clear the persistent store
                    for message in messages:
                        self.memory.add_message(message)  # Re-save the updated list

                    self.in_memory = messages
                    
                    self.utils.bot_log(self.name, {"role": "admin", "content": "Updated system prompt."})
                elif hasattr(self.memory, "chat_memory") and self.memory.chat_memory.messages:
                    messages = self.memory.chat_memory.messages  # Fetch current messages
                    if len(messages) > 0:
                        messages[0] = {"role": "system", "content": self.system_prompt}
                    else:
                        messages = [{"role": "system", "content": self.system_prompt}]
                    self.memory.clear()       # Clear the persistent store
                    for message in messages:
                        self.memory.add_message(message)  # Re-save the updated list

                    self.in_memory = messages
                    
                    self.utils.bot_log(self.name, {"role": "admin", "content": "Updated system prompt."})
            
                else:
                    raise ValueError('The memory instance has no attribute messages.')
            else:
                if self.in_memory and len(self.in_memory) > 0:
                    self.in_memory[0] = {"role": "system", "content": self.system_prompt}
                else:
                    self.in_memory = [{"role": "system", "content": self.system_prompt}]
                        
                self.utils.bot_log(self.name, {"role": "admin", "content": "Updated system prompt."})
            

    def add_message(self, content, role='assistant', remove_linebreaks=False):
        """
        Add a message to memory, if memory is enabled.

        Parameters:
        - role (str): The role of the message sender (e.g., "user", "assistant").
        - content (str): The message content.
        """
        if self.memory:
            self.memory.add_message(role, content)
            
        self.last_in_memory = content
        cleaned_message = self.utils.clean_text(content, remove_linebreaks=remove_linebreaks)
        if self.in_memory:
            self.in_memory.append({"role": role, "content": cleaned_message})
        else:
            self.in_memory = [{"role": role, "content": cleaned_message}]
        
        self.utils.bot_log(self.name, cleaned_message)
        
        if self.verbose:
            print(f"[{role}] {content}")
            
    def add_response(self, message, role='assistant', remove_linebreaks=False):
        """
        Temporary patch.
        """
        self.add_message(message, role=role, remove_linebreaks=remove_linebreaks)

    def clear_memory(self):
        """
        Clear the stored memory.
        """
        self.reset(clear = True)
        
    def remove(self, index=-1, query_and_response=False):
        """
        Remove a message from memory.

        Args:
            index (int): Index of the message to remove.
            query_and_response (bool): Whether to remove both query and response.
        """
        
        if self.memory:
            if hasattr(self.memory, "messages"):
                messages = self.memory.messages  # Fetch current messages
                if index < len(messages) and len(messages) > 0:
                    messages.pop(index)  # Remove the message
                if query_and_response and index < len(messages) and len(messages) > 0:
                    messages.pop(index)  # Remove the message
                self.memory.clear()       # Clear the persistent store
                for message in messages:
                    self.memory.add_message(message)  # Re-save the updated list
                
                self.in_memory = messages
            
            elif hasattr(self.memory, "chat_memory") and self.memory.chat_memory.messages:
                messages = self.memory.chat_memory.messages  # Fetch current messages
                if index < len(messages) and len(messages) > 0:
                    messages.pop(index)  # Remove the message
                if query_and_response and index < len(messages) and len(messages) > 0:
                    messages.pop(index)  # Remove the message
                self.memory.clear()       # Clear the persistent store
                for message in messages:
                    self.memory.add_message(message)  # Re-save the updated list
                
                self.in_memory = messages
            else:
                raise ValueError('The memory instance has no attribute messages.')
    
        elif self.in_memory and index < len(self.in_memory):
            del self.in_memory[index]
            self.utils.bot_log(self.name, {"role": "admin", "content": "Bot removed memory."})

            if query_and_response and index < len(self.in_memory) and self.in_memory:
                del self.in_memory[index]
                self.utils.bot_log(self.name, {"role": "admin", "content": "Bot removed memory again."})

    def chat(self, q=None, as_json=False, reset=False, erase_query=False, remove_linebreaks=False):
        """
        Send a query to the LLM and receive a response.

        Args:
            q (str): Query to send.
            as_json (bool): Whether to return the response as JSON.
            reset (bool): Whether to reset memory before sending the query.
            erase_query (bool): Whether to remove the query from memory after processing.
            remove_linebreaks (bool): Whether to remove line breaks from the query.

        Returns:
            str: The response from the LLM.
        """
        if reset:
            self.reset()

        if q:
            self.add_message(q, role='user', remove_linebreaks=remove_linebreaks)

        if self.provider == 'langchain-openai':
            if self.memory and BaseMemory is not None and isinstance(self.memory, BaseMemory):
                response = self.agent.run(q).content
            else:
                response = self.agent.invoke(self.in_memory).content
        elif self.provider == 'openai':
            try:
                completion = self.agent.ChatCompletion.create(
                    model=self.model,
                    messages=self.in_memory,
                    temperature=0.0
                )
                response = completion['choices'][0]['message']['content']
            except:
                completion = self.agent.chat.completions.create(
                    model=self.model,
                    messages=self.in_memory,
                    temperature=0.0
                )
                response = completion.choices[0].message.content
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

        if q and erase_query:
            self.remove()

        self.utils.bot_log(self.name, response)

        if self.utils:
            self.utils.update_price_tokens_use_estimates(str(self.in_memory) + response, model=self.model, verbose=False)

        return response

    def reset(self, clear = False):
        """
        Reset the LLM state, including clearing memory and reset the system prompt.
        """
        if self.memory:
            self.memory.clear()
        
        self.utils.bot_log(self.name, {"role": "admin", "content": "Bot was reset."})
        self.in_memory = []
        self.last_in_memory = ''
                                                      
        if not clear:
            self.update_system_prompt()
        
        if self.verbose:
            print(f"Resetting {self.name} state.")

    def share_conversation(self):
        """
        Share the current conversation as a JSON string.

        Returns:
            str: JSON string of conversation memory (excluding system prompt).
        """
        return "".join(json.dumps(x) for x in self.in_memory[1:]) if self.in_memory else ""

    def get_last_in_memory(self):
        """
        Get the last message stored in memory.

        Returns:
            str: Content of the last message.
        """
        return self.last_in_memory or (self.in_memory[-1]['content'] if self.in_memory else "")

    def get_memory(self, start=1, stop=None):
        """
        Retrieve a subset of the conversation memory.

        Args:
            start (int): Start index for the memory slice.
            stop (int): End index for the memory slice.

        Returns:
            list: Subset of the conversation memory.
        """
        if self.memory:
            if hasattr(self.memory, "chat_memory") and self.memory.chat_memory.messages:
                return self.memory.chat_memory.messages[start:stop]
            elif hasattr(self.memory, "messages") and self.memory.messages:
                return self.memory.messages[start:stop]
            else:
                return []

        return self.in_memory[start:stop] if self.in_memory else []

    def set_memory(self, memory, clear=True):
        """
        Set the conversation memory.

        Args:
            memory (list): List of memory items to set.
            clear (bool): Whether to clear existing memory before setting.
        """
        
        if self.memory:
            if clear:
                self.memory.clear()
                self.in_memory = messages
                self.utils.bot_log(self.name, {"role": "admin", "content": "Cleared all memories."})
            else:
                self.in_memory.extend(memory)
                
            for mem in memory:
                self.memory.add_message(mem)
                self.utils.bot_log(self.name, mem)
            
        else:
            if clear:
                self.in_memory = [self.in_memory[0]] + memory if self.in_memory else memory
                self.utils.bot_log(self.name, {"role": "admin", "content": "Cleared all memories."})
            else:
                self.in_memory.extend(memory)
            
            for mem in memory:
                self.utils.bot_log(self.name, mem)


